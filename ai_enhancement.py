import json
import time
import os
import pandas as pd
from typing import List, Dict, Any, Optional
import logging
from datetime import datetime
from google import genai
from google.genai import types
from llm_output_parser import parse_json
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

logger = logging.getLogger(__name__)

class AIEnhancementProcessor:
    def __init__(self, config: Dict[str, Any]):
        """Initialize AI enhancement processor."""
        
        load_dotenv()

        self.api_key = os.getenv('GOOGLE_API_KEY')
        self.model_name = config.get('model', 'gemini-2.5-flash-lite')
        self.temperature = config.get('temperature', 0.7)
        self.batch_size = config.get('batch_size', 50)
        self.retry_delay = config.get('retry_delay', 60)
        self.retry_attempts = config.get('retry_attempts', 3)
        self.max_parallel_calls = config.get('max_parallel_calls', 5)

        # Initialize the Gemini API
        self.client = genai.Client(api_key=self.api_key)
        grounding_tool = types.Tool(
            google_search=types.GoogleSearch()
        )

        self.config = types.GenerateContentConfig(
            tools=[grounding_tool],
            system_instruction=self.create_system_prompt(),
            temperature=self.temperature
        )
        
        # Quota tracking
        self.calls_lock = threading.Lock()
        self.calls_in_current_minute = 0
        self.tokens_in_current_minute = 0
        self.minute_start_time = time.time()
        
        # Ensure tmp directory exists
        self.tmp_dir = os.path.join(os.path.dirname(__file__), 'tmp')
        os.makedirs(self.tmp_dir, exist_ok=True)

    def prepare_batch_data(self, df: pd.DataFrame, start_idx: int, end_idx: int) -> List[Dict[str, str]]:
        """Prepare batch data for AI processing."""
        batch_df = df.iloc[start_idx:end_idx].copy()
        
        # Filter and prepare data
        products = []
        for _, row in batch_df.iterrows():
            if pd.isna(row.get('Spracovane AI', False)) or not row.get('Spracovane AI', False):
                product = {
                    "NÃ¡zov tovaru": str(row.get('NÃ¡zov tovaru', '')),
                    "Hlavna kategÃ³ria": str(row.get('Hlavna kategÃ³ria', '')),
                    "KrÃ¡tky popis": str(row.get('KrÃ¡tky popis', '')),
                    "DlhÃ½ popis": str(row.get('DlhÃ½ popis', ''))
                }
                products.append(product)
        
        return products

    def _check_and_wait_for_quota(self, tokens_needed: int = 0):
        """Check quota and wait if necessary."""
        with self.calls_lock:
            current_time = time.time()
            
            # Reset counters if a minute has passed
            if current_time - self.minute_start_time >= 60:
                self.calls_in_current_minute = 0
                self.tokens_in_current_minute = 0
                self.minute_start_time = current_time
            
            # Check if we need to wait
            need_to_wait = False
            wait_reason = ""
            
            # Check call limit (15 calls per minute)
            if self.calls_in_current_minute >= 15:
                need_to_wait = True
                wait_reason = "call limit"
            
            # Check token limit (250,000 tokens per minute)
            if self.tokens_in_current_minute + tokens_needed > 250000:
                need_to_wait = True
                wait_reason = "token limit"
            
            if need_to_wait:
                # Calculate wait time until next minute
                time_to_wait = 60 - (current_time - self.minute_start_time)
                if time_to_wait > 0:
                    logger.info(f"Quota limit reached ({wait_reason}), waiting {time_to_wait:.1f} seconds until next minute...")
                    time.sleep(time_to_wait)
                
                # Reset counters
                self.calls_in_current_minute = 0
                self.tokens_in_current_minute = 0
                self.minute_start_time = time.time()
            
            # Increment counters
            self.calls_in_current_minute += 1
            self.tokens_in_current_minute += tokens_needed

    def process_batch_with_retry(self, products: List[Dict[str, str]]) -> Optional[List[Dict[str, str]]]:
        """Process a batch of products with retry logic."""
        if not products:
            return []
        
        # Estimate tokens needed (rough estimation)
        estimated_tokens = len(json.dumps(products)) * 1.5
        self._check_and_wait_for_quota(int(estimated_tokens))
        
        for attempt in range(self.retry_attempts):
            try:
                # Prepare prompt
                user_prompt = json.dumps(products, ensure_ascii=False, indent=None)
                
                # Send to Gemini API
                response = self.client.models.generate_content(
                    model=self.model_name,
                    config=self.config,
                    contents=user_prompt
                    )
                
                # Track actual tokens used
                if hasattr(response, 'usage_metadata') and response.usage_metadata:
                    actual_tokens = response.usage_metadata.total_token_count
                    with self.calls_lock:
                        # Adjust token count with actual usage
                        self.tokens_in_current_minute = self.tokens_in_current_minute - estimated_tokens + actual_tokens
                
                # Parse response
                if response and response.text:
                    # content = response.text.strip().replace('```json', '').replace('```', '').replace('\n', '')
                    
                    # Try to parse JSON response
                    try:
                        enhanced_products = parse_json(response.text)
                        return enhanced_products
                    except json.JSONDecodeError:
                        # If response is not valid JSON, try to extract JSON from text
                        if '[' in response.text and ']' in response.text:
                            json_start = response.text.find('[')
                            json_end = response.text.rfind(']') + 1
                            json_str = response.text[json_start:json_end]
                            enhanced_products = json.loads(json_str)
                            return enhanced_products
                        
                logger.error(f"Invalid response format: {response.text if response else 'No response'}")
                return None
                
            except Exception as e:
                if "rate limit" in str(e).lower() or "quota" in str(e).lower():
                    logger.warning(f"Rate limit hit, waiting {self.retry_delay} seconds...")
                    time.sleep(self.retry_delay)
                    continue
                else:
                    logger.error(f"Error processing batch: {e}")
                    if attempt < self.retry_attempts - 1:
                        time.sleep(2 ** attempt)  # Exponential backoff
                        continue
                    raise
        
        return None

    def update_dataframe(self, df: pd.DataFrame, enhanced_products: List[Dict[str, str]]) -> pd.DataFrame:
        """Update dataframe with enhanced descriptions."""
        df = df.copy()
        
        for enhanced_product in enhanced_products:
            df.loc[df['NÃ¡zov tovaru'] == enhanced_product['NÃ¡zov tovaru'], [
                'KrÃ¡tky popis', 'DlhÃ½ popis', 'SEO titulka', 'SEO popis', 'SEO kÄ¾ÃºÄovÃ© slovÃ¡',
                'Spracovane AI', 'AI_Processed_Date'
            ]] = [
                enhanced_product['KrÃ¡tky popis'], enhanced_product['DlhÃ½ popis'], enhanced_product['SEO titulka'],
                enhanced_product['SEO popis'], enhanced_product['SEO kÄ¾ÃºÄovÃ© slovÃ¡'], True, datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            ]
        
        return df

    def process_dataframe(self, df: pd.DataFrame, progress_callback=None) -> pd.DataFrame:
        """Process entire dataframe with AI enhancement using parallel processing."""
        if not self.api_key:
            logger.warning("No API key provided, skipping AI enhancement")
            return df
        
        # Add AI processing columns if they don't exist
        if 'Spracovane AI' not in df.columns:
            df['Spracovane AI'] = False
        if 'AI_Processed_Date' not in df.columns:
            df['AI_Processed_Date'] = ''
        
        # Filter products needing processing
        needs_processing = df[df['Spracovane AI'].isin([False, 'FALSE', ""])]
        total_products = len(needs_processing)
        
        if total_products == 0:
            logger.info("No products need AI enhancement")
            return df
        
        logger.info(f"Processing {total_products} products with AI enhancement")
        
        processed_count = 0
        
        # Process in batches with parallel execution
        batches = []
        for i in range(0, total_products, self.batch_size):
            batch_end = min(i + self.batch_size, total_products)
            # Get indices from original dataframe
            original_indices = needs_processing.index[i:batch_end]
            # Prepare batch data
            products = self.prepare_batch_data(df, original_indices[0], original_indices[-1] + 1)
            if products:
                batches.append((products, original_indices[0], len(batches) + 1))
        
        # Process batches in parallel
        with ThreadPoolExecutor(max_workers=self.max_parallel_calls) as executor:
            # Submit all batches
            future_to_batch = {
                executor.submit(self._process_single_batch, batch_data, start_idx, batch_num): batch_num
                for batch_data, start_idx, batch_num in batches
            }
            
            # Process completed batches
            for future in as_completed(future_to_batch):
                batch_num = future_to_batch[future]
                try:
                    result = future.result()
                    if result:
                        enhanced_products = result[0]
                        df = self.update_dataframe(df, enhanced_products)
                        processed_count += len(enhanced_products)
                        
                        # Save incremental progress
                        tmp_file = os.path.join(self.tmp_dir, 'processed_tmp.csv')
                        try:
                            # First try cp1250 with character replacement
                            try:
                                for col in df.columns:
                                    if df[col].dtype == 'object':
                                        df[col] = df[col].astype(str)
                                        df[col] = df[col].apply(
                                            lambda x: ''.join(c if c.encode('cp1250', errors='replace') != b'?' else ' ' for c in x)
                                        )
                                
                                df.to_csv(tmp_file, index=False, encoding='cp1250', sep=';')
                            except Exception:
                                # Fallback to utf-8 if cp1250 fails
                                df.to_csv(tmp_file, index=False, encoding='utf-8', sep=';')
                            
                            logger.info(f"Saved incremental progress for batch {batch_num} to {tmp_file}")
                        except Exception as e:
                            logger.error(f"Failed to save incremental progress: {e}")
                        
                        logger.info(f"Processed batch {batch_num}/{len(batches)}")
                except Exception as e:
                    logger.error(f"Error processing batch {batch_num}: {e}")
                
                # Update progress
                if progress_callback:
                    progress_callback(processed_count, total_products)
        
        # Final save after all processing
        tmp_file = os.path.join(self.tmp_dir, 'processed_tmp.csv')
        try:
            # First try cp1250 with character replacement
            try:
                for col in df.columns:
                    if df[col].dtype == 'object':
                        df[col] = df[col].astype(str)
                        df[col] = df[col].apply(
                            lambda x: ''.join(c if c.encode('cp1250', errors='replace') != b'?' else ' ' for c in x)
                        )
                
                df.to_csv(tmp_file, index=False, encoding='cp1250', sep=';')
            except Exception:
                # Fallback to utf-8 if cp1250 fails
                df.to_csv(tmp_file, index=False, encoding='utf-8', sep=';')
                
            logger.info(f"Saved final progress to {tmp_file}")
        except Exception as e:
            logger.error(f"Failed to save final progress: {e}")
            
        logger.info(f"AI enhancement completed. Processed {processed_count} products")
        return df

    def _process_single_batch(self, products: List[Dict[str, str]], start_idx: int, batch_num: int):
        """Process a single batch of products."""
        enhanced_products = self.process_batch_with_retry(products)
        if enhanced_products:
            return enhanced_products, start_idx
        return None

    def create_system_prompt(self) -> str:
        """Create system prompt for AI enhancement."""
        return """Si Å¡pecializovanÃ½ AI expert copywriter, SEO konzultant a technickÃ½ poradca pre e-shopy s profesionÃ¡lnym gastro vybavenÃ­m, nÃ¡radÃ­m a zariadeniami.

        Tvojou Ãºlohou je:

        1. **vylepÅ¡iÅ¥ alebo doplniÅ¥ produktovÃ© popisy** (krÃ¡tky + dlhÃ½ popis) pre B2B cieÄ¾ovku (reÅ¡taurÃ¡cie, hotely, kantÃ­ny, vÃ½robnÃ© kuchyne),
        2. **vygenerovaÅ¥ profesionÃ¡lne SEO meta Ãºdaje** â€“ SEO titulku, SEO popis a SEO kÄ¾ÃºÄovÃ© slovÃ¡.

        ---

        ### ğŸ“¥ **VSTUP**

        DostaneÅ¡ vstup ako **JSON pole** s nasledovnou Å¡truktÃºrou:

        ```json
        [
        {
            "NÃ¡zov tovaru": "NÃ¡zov produktu",
            "Hlavna kategÃ³ria": "Hlavna kategÃ³ria/Podkategoria/Podkategoria",
            "KrÃ¡tky popis": "StruÄnÃ½ existujÃºci popis",
            "DlhÃ½ popis": "DetailnÃ½ popis alebo prÃ¡zdne pole"
        }
        ]
        ```

        ---

        ### âœï¸ **TVOJA ÃšLOHA PRE KAÅ½DÃ PRODUKT**

        #### ğŸ”¹ 1. **KrÃ¡tky popis** (50â€“100 slov)

        * ZhrÅˆ zÃ¡kladnÃº funkciu a pouÅ¾itie
        * UveÄ dÃ´leÅ¾itÃ© parametre (vÃ½kon, rozmery, materiÃ¡ly)
        * ZdÃ´razni hlavnÃº konkurenÄnÃº vÃ½hodu
        * UveÄ typ cieÄ¾ovej prevÃ¡dzky
        * PouÅ¾i **HTML znaÄky** (`<strong>`, `<br>`, `<ul>`, `<li>`, '<p> atÄ.)

        #### ğŸ”¹ 2. **DlhÃ½ popis** (200â€“400 slov)

        * Å truktÃºra:

        * ÃšvodnÃ½ odstavec â€“ pozicionovanie a ÃºÄel produktu
        * TechnickÃ© vlastnosti â€“ vÃ½kony, rozmery, kapacita, materiÃ¡ly
        * VÃ½hody pre prevÃ¡dzku â€“ Ãºspora Äasu, energie, Å¡tandardizÃ¡cia, produktivita
        * InÅ¡talÃ¡cia a ÃºdrÅ¾ba â€“ pripojenie, Äistenie, servis
        * ZÃ¡ver â€“ certifikÃ¡cie, odporÃºÄanÃ© pouÅ¾itie
        * PouÅ¾i HTML znaÄky (`<p>`, `<ul>`, `<li>`, `<strong>` atÄ.)
        * Prirodzene zaÄleÅˆ SEO frÃ¡zy:

        * â€profesionÃ¡lne gastro vybavenieâ€œ
        * â€komerÄnÃ¡ kuchyÅˆa \\ [typ zariadenia]â€œ
        * â€horeca \\ [kategÃ³ria]â€œ
        * â€\\ [znaÄka] \\ [model] technickÃ© parametreâ€œ
        * UvÃ¡dzaj technickÃ© Ãºdaje (vÃ½kon, kapacita, materiÃ¡ly, rozmery)
        * Ak je produkt nejasnÃ½, **pouÅ¾i webovÃ© vyhÄ¾adÃ¡vanie** na zistenie funkcie a parametrov (simuluj odbornÃ© overenie informÃ¡ciÃ­)

        ---

        ### ğŸ” **SEO META ÃšDAJE â€“ VYGENERUJ TIEÅ½**

        #### âœ… SEO titulka

        * DÄºÅ¾ka: 45â€“70 znakov
        * Obsahuje nÃ¡zov produktu/sluÅ¾by + znaÄka, kategÃ³ria alebo unikÃ¡tna vÃ½hoda
        * KaÅ¾dÃ¡ SEO titulka musÃ­ byÅ¥ jedineÄnÃ¡
        * Pridaj suffix "| GastroPro.sk"
        * PrÃ­klad: â€PracovnÃ½ stÃ´l GN1/1 so zÃ¡suvkami â€“ nerezovÃ½ nÃ¡bytok | GastroPro.skâ€œ

        #### âœ… SEO popis

        * DÄºÅ¾ka: 120â€“160 znakov
        * Obsahuje vÃ½hody, kÄ¾ÃºÄovÃ© parametre alebo pouÅ¾itie
        * Motivuje k akcii (napr. Objednajte online, VyskÃºÅ¡ajte zdarma, Zistite viac)
        * PrÃ­klad: â€RobustnÃ½ nerezovÃ½ stÃ´l GN1/1 so zÃ¡suvkami pre gastro prevÃ¡dzky. VysokÃ¡ odolnosÅ¥, hygienickÃ© spracovanie, rÃ½chle dodanie.â€œ

        #### âœ… SEO kÄ¾ÃºÄovÃ© slovÃ¡

        * 3â€“7 relevantnÃ½ch vÃ½razov oddelenÃ½ch Äiarkou
        * PrÃ­klad: â€nerezovÃ½ pracovnÃ½ stÃ´l, GN1/1 stÃ´l, gastro nÃ¡bytok, horeca vybavenie, profesionÃ¡lna kuchyÅˆaâ€œ

        ---

        ### ğŸ“¤ **VÃSTUP**

        **Presne to istÃ© JSON pole** s vÅ¡etkÃ½mi produktmi ale s vylepÅ¡enÃ½mi poÄ¾ami:

        * `"KrÃ¡tky popis"` (HTML),
        * `"DlhÃ½ popis"` (HTML),
        * `"SEO titulka"`,
        * `"SEO popis"`,
        * `"SEO kÄ¾ÃºÄovÃ© slovÃ¡"`.

        **Bez poÄ¾a `"Hlavna kategÃ³ria"`**.

        **VÃ½stup musÃ­ byÅ¥ IBA ÄistÃ© JSON pole â€“ Å¾iadne komentÃ¡re, vysvetlenia, ÃºvodnÃ½ ani zÃ¡vereÄnÃ½ text.**

        ```json
        [
        {
            "NÃ¡zov tovaru": "NÃ¡zov produktu",
            "KrÃ¡tky popis": "<strong>ProfesionÃ¡lny ...</strong><br>...",
            "DlhÃ½ popis": "<p>...</p><ul><li>...</li></ul>",
            "SEO titulka": "....",
            "SEO popis": "....",
            "SEO kÄ¾ÃºÄovÃ© slovÃ¡": "..."
        }
        ]
        ```

        ---

        ### âœ… **KONTROLA PRED VÃSTUPOM**

        * [ ] Popisy sÃº profesionÃ¡lne a technicky sprÃ¡vne
        * [ ] ObsahujÃº HTML znaÄky
        * [ ] ObsahujÃº relevantnÃ© SEO prvky (title, description, keywords)
        * [ ] Nie sÃº prÃ­tomnÃ© Å¾iadne duplicity ani nerelevantnÃ© frÃ¡zy
        * [ ] DÄºÅ¾ky SEO prvkov sÃº dodrÅ¾anÃ©
        * [ ] VÃ½stup je ÄistÃ½ JSON bez akÃ½chkoÄ¾vek inÃ½ch prvkov
        """